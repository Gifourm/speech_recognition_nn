{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM1YVtTlnKl38/Jyoi5bfO3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gifourm/speech_recognition_nn/blob/empty_branch/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6OblmVRsJQ_v",
        "outputId": "f2c157ba-ecf5-48d0-b283-7e588ef75d97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "150/150 [==============================] - 27s 174ms/step - loss: 1.7570 - val_loss: 1.2938\n",
            "Epoch 2/10\n",
            "144/150 [===========================>..] - ETA: 0s - loss: 1.2061"
          ]
        }
      ],
      "source": [
        ", test\n",
        "import os\n",
        "import pathlib\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy\n",
        "import seaborn as sns\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import models, Model\n",
        "from IPython import display\n",
        "\n",
        "\n",
        "class Network(Model):\n",
        "  def __init__(self):\n",
        "    super(Network, self).__init__()\n",
        "    self.input_layer = layers.Input(shape=input_shape)\n",
        "    self.resizing = layers.Resizing(32, 32)\n",
        "    self.normalize = norm_layer\n",
        "    self.conv1 = layers.Conv2D(32, 3, activation='relu')\n",
        "    self.conv2 = layers.Conv2D(64, 3, activation='relu')\n",
        "    self.pool = layers.MaxPooling2D()\n",
        "    self.dropout1 = layers.Dropout(0.25)\n",
        "    self.flatten = layers.Flatten()\n",
        "    self.d1 = layers.Dense(128, activation='relu')\n",
        "    self.dropout2 = layers.Dropout(0.5)\n",
        "    self.d2 = layers.Dense(len(words))\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.resizing(x)\n",
        "    x = self.normalize(x)\n",
        "    x = self.conv1(x)\n",
        "    x = self.conv2(x)\n",
        "    x = self.pool(x)\n",
        "    x = self.dropout1(x)\n",
        "    x = self.flatten(x)\n",
        "    x = self.d1(x)\n",
        "    x = self.dropout2(x)\n",
        "    return self.d2(x)\n",
        "\n",
        "\n",
        "def decode_wav_file(audio_binary):\n",
        "  audio, _ = tf.audio.decode_wav(contents=audio_binary)\n",
        "  return tf.squeeze(audio, axis=-1)\n",
        "\n",
        "\n",
        "def get_labels(file_path):\n",
        "  label = tf.strings.split(\n",
        "      input=file_path,\n",
        "      sep=os.path.sep\n",
        "  )\n",
        "  return label[-2]\n",
        "\n",
        "\n",
        "def get_ds(file_path):\n",
        "  label = get_labels(file_path)\n",
        "  audio_binary = tf.io.read_file(file_path)\n",
        "  decode_wav = decode_wav_file(audio_binary)\n",
        "  return decode_wav, label\n",
        "\n",
        "\n",
        "def spectrogram(wave):\n",
        "  input_len = 16000\n",
        "  wave = wave[:input_len]\n",
        "  padding = tf.zeros([input_len] - tf.shape(wave), dtype=tf.float32)\n",
        "\n",
        "  wave = tf.cast(wave, dtype=tf.float32)\n",
        "  len = tf.concat([wave, padding], 0)\n",
        "\n",
        "  specter = tf.abs(tf.signal.stft(len, frame_length=255, frame_step=128))\n",
        "\n",
        "  specter = specter[..., tf.newaxis]\n",
        "  return specter\n",
        "\n",
        "\n",
        "def display_spectrogram(spectrogram, ax):\n",
        "  if len(spectrogram.shape) > 2:\n",
        "    assert len(spectrogram.shape) == 3\n",
        "    spectrogram = numpy.squeeze(spectrogram, axis=-1)\n",
        "  log_specter = numpy.log(spectrogram.T + numpy.finfo(float).eps)\n",
        "  height = log_specter.shape[0]\n",
        "  width = log_specter.shape[1]\n",
        "  x = numpy.linspace(0, numpy.size(spectrogram), num=width, dtype=int)\n",
        "  y = range(height)\n",
        "  ax.pcolormesh(x, y, log_specter)\n",
        "\n",
        "\n",
        "def get_specter_and_label(audio, label):\n",
        "  specter = spectrogram(audio)\n",
        "  label = tf.argmax(label == words)\n",
        "  return specter, label\n",
        "\n",
        "\n",
        "def preprocessing(files):\n",
        "  ds = tf.data.Dataset.from_tensor_slices(files)\n",
        "  output_ds = ds.map(\n",
        "      map_func=get_ds,\n",
        "      num_parallel_calls=tf.data.AUTOTUNE\n",
        "  )\n",
        "  output_ds = output_ds.map(\n",
        "      map_func=get_specter_and_label,\n",
        "      num_parallel_calls=tf.data.AUTOTUNE\n",
        "  )\n",
        "  return output_ds\n",
        "\n",
        "\n",
        "path = 'data/mini_speech_commands'\n",
        "tf.keras.utils.get_file(\n",
        "    'mini_speech_commands.zip',\n",
        "    origin='http://storage.googleapis.com/download.tensorflow.org/data/mini_speech_commands.zip',\n",
        "    extract=True,\n",
        "    cache_dir='.',\n",
        "    cache_subdir='data'\n",
        ")\n",
        "\n",
        "words = numpy.array(tf.io.gfile.listdir(str(pathlib.Path(path))))\n",
        "words = words[words != 'README.md']\n",
        "\n",
        "files = tf.io.gfile.glob(str(path) + '/*/*')\n",
        "files = tf.random.shuffle(files)\n",
        "\n",
        "train_ds = files[:int(len(files) / 10 * 6)]\n",
        "val_ds = files[int(len(files) / 10 * 6):int(len(files) / 10 * 8)]\n",
        "test_ds = files[-int(len(files) / 10 * 2):]\n",
        "\n",
        "ds = tf.data.Dataset.from_tensor_slices(train_ds)\n",
        "\n",
        "waves_ds = ds.map(\n",
        "    map_func=get_ds,\n",
        "    num_parallel_calls=tf.data.AUTOTUNE\n",
        ")\n",
        "\n",
        "for wave, label in waves_ds.take(1):\n",
        "  label = label.numpy().decode('utf-8')\n",
        "  specter = spectrogram(wave)\n",
        "\n",
        "specter_ds = waves_ds.map(\n",
        "    map_func=get_specter_and_label,\n",
        "    num_parallel_calls=tf.data.AUTOTUNE\n",
        ")\n",
        "\n",
        "train_ds = specter_ds.batch(32).cache().prefetch(tf.data.AUTOTUNE)\n",
        "val_ds = preprocessing(val_ds).batch(32).cache().prefetch(tf.data.AUTOTUNE)\n",
        "test_ds = preprocessing(test_ds)\n",
        "\n",
        "for specter, _ in specter_ds.take(1):\n",
        "  input = specter.shape\n",
        "\n",
        "for spectrogram, _ in specter_ds.take(1):\n",
        "  input_shape = spectrogram.shape\n",
        "\n",
        "num_labels = len(words)\n",
        "\n",
        "norm_layer = layers.Normalization()\n",
        "norm_layer.adapt(data=specter_ds.map(map_func=lambda spec, label: spec))\n",
        "\n",
        "model = Network()\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='val_accuracy')\n",
        "model.compile(optimizer, loss_object)\n",
        "\n",
        "EPOCHS = 10\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=tf.keras.callbacks.EarlyStopping(verbose=1, patience=2)\n",
        ")\n",
        "\n",
        "test_audio = numpy.array([audio.numpy() for audio, _ in test_ds])\n",
        "test_labels = numpy.array([label.numpy() for _, label in test_ds])\n",
        "\n",
        "predictions = numpy.argmax(model.predict(test_audio), axis=1)\n",
        "label = test_labels\n",
        "\n",
        "accuracy = sum(predictions == label) / len(label)\n",
        "print(accuracy)\n",
        "\n",
        "test_audio = numpy.array([audio.numpy() for audio, _ in test_ds.skip(80).take(1)])\n",
        "test_label = numpy.array([label.numpy() for _, label in test_ds.skip(80).take(1)])\n",
        "\n",
        "predictions = numpy.argmax(model.predict(test_audio), axis=1)\n",
        "label = test_labels\n",
        "\n",
        "accuracy = sum(predictions == label) / len(label)\n",
        "print(f'Prediction: {words[predictions[0]]}\\nLabel: {words[label[0]]}')"
      ]
    }
  ]
}